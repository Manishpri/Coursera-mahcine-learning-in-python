# -*- coding: utf-8 -*-
"""single_feature_linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16f7vrIduklYRoa_3JYPRk_87YbwFsxaL
"""

#Import Library
import numpy as np
import matplotlib.pyplot as plt
import csv
import io
from scipy.stats import linregress
from sklearn import linear_model
import pylab
from google.colab import files
import pandas as pd
from google.colab import files
uploaded = files.upload()

#Fetching training data
data = np.loadtxt('ex1data1.txt', delimiter=",")
X = data[:,:1]
y = data[:,1]

#Scattering training data

#Scattering training data
plt.scatter(X, y, marker='o', c='b')
plt.title('Profits distribution',y =1.07)
plt.xlabel('Population of City in 10,000s',labelpad = 20)
plt.ylabel('Profit in $10,000s',labelpad = 20)
plt.show()

#Reshape training data(it's because single feature)
X = np.array(X)
X = X.reshape(-1,1)
y = np.array(y)
y = y.reshape(-1,1)

#number of training samples
m = y.size

#Add a column of ones to X (interception data)
X = np.c_[np.ones(m), X]

#Initialize theta parameters
theta = np.zeros(shape=(2, 1))

#Some gradient descent settings
num_iters = 5000
alpha = 0.02

#Evaluate cost
def compute_cost(X, y, theta):
    
    #Number of training samples
    m = y.size
    
    #Hypothesis
    h = np.dot(X,theta)
    
    #Square difference
    sqErrors = np.square(h - y)
    
    #Error cost
    J =sqErrors.sum()/(2 * m)
    return J

# Evaluate Gradient Descent(Minimizing cost function)
def gradient_descent(X, y, theta, alpha, num_iters):
    m = y.size
    
    #Initialize J_hisotry
    J_history = np.zeros(num_iters)
    
    for iter in np.arange(num_iters):
      
        #Hypothesis
        h = X.dot(theta)
        
        #Updating theta
        theta = theta - alpha*(1/m)*(X.T.dot(h-y))
        
        #Computing cost for every theta
        J_history[iter] = compute_cost(X, y, theta)
        
    return(theta, J_history)

#Testing cost function with theta [0;0]
J = compute_cost(X, y, theta)
print('cost with theta[0;0]:',J)

#Testing cost function with theta [-1;2]
J = compute_cost(X, y,[[-1],[2]])
print('cost with theta[-1;2]:',J)

#Theta found by gradientDescent
theta, J_history = gradient_descent(X, y, theta, alpha, num_iters)
print(theta)

"""Here theta computed by Gradient Descent and Normal Equation is same."""

#Theta by Normal equation
theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
print(theta)

#For population = 35,000, we predict a profit of 2798.36876352
print(np.array([1, 3.5]).dot(theta)*10000)
#For population = 70,000, we predict a profit of 44554.54631015       
print(np.array([1, 7]).dot(theta) * 10000)

#Plot the results
result = np.dot(X,theta).flatten()
plt.scatter(data[:,:1], data[:,1], marker='o', c='b')
plt.plot(data[:,:1], result, c = 'r')
plt.title('Profits distribution',y =1.07)
plt.xlabel('Population of City in 10,000s',labelpad=20)
plt.ylabel('Profit in $10,000s',labelpad=20)
plt.show()

#Convergence of gradient descent with an appropriate learning rate
plt.plot(J_history[0:1000])
plt.ylabel('Cost J')
plt.xlabel('Iterations');

from mpl_toolkits.mplot3d import axes3d
# Create grid coordinates for plotting
B0 = np.linspace(-10, 10, 50)
B1 = np.linspace(-1, 4, 50)
xx, yy = np.meshgrid(B0, B1, indexing='xy')
Z = np.zeros((B0.size,B1.size))

# Calculate Z-values (Cost) based on grid of coefficients
for (i,j),v in np.ndenumerate(Z):
    Z[i,j] = compute_cost(X,y, theta=[[xx[i,j]], [yy[i,j]]])

fig = plt.figure(figsize=(15,6))
ax1 = fig.add_subplot(121)
ax2 = fig.add_subplot(122, projection='3d')

# Left plot
CS = ax1.contour(xx, yy, Z, np.logspace(-2, 3, 20), cmap=plt.cm.jet)
ax1.scatter(theta[0],theta[1], c='r')

# Right plot
ax2.plot_surface(xx, yy, Z, rstride=1, cstride=1, alpha=0.6, cmap=plt.cm.jet)
ax2.set_zlabel('Cost')
ax2.set_zlim(Z.min(),Z.max())
ax2.view_init(elev=15, azim=230)
# settings common to both plots
for ax in fig.axes:
    ax.set_xlabel(r'$\theta_0$', fontsize=17)
    ax.set_ylabel(r'$\theta_1$', fontsize=17)

#Linear regression with library function

from sklearn.linear_model import LinearRegression

#Creating the object of regression
regr = linear_model.LinearRegression()

#Train the model using the training sets
regr.fit(data[:,:1], data[:,1])

plt.figure(figsize=(8,6))


#Plot outputs
plt.scatter(data[:,:1], data[:,1],color='black')
plt.title('Profits distribution')
plt.xlabel('Population of City in 10,000s')
plt.ylabel('Profit in $10,000s')
plt.plot(data[:,:1],regr.predict(data[:,:1]), color='red',linewidth=3)
predicted = regr.predict(7)
print(predicted*10000)

