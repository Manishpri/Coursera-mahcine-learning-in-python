# -*- coding: utf-8 -*-
"""linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gjv_za2y_m1B3EswY4Lm1R1cNePbN0Hk
"""

#Import Library
import numpy as np
import matplotlib.pyplot as plt
import csv
import io
from scipy.stats import linregress
from sklearn import linear_model
import pylab
from google.colab import files
import pandas as pd
from google.colab import files
uploaded = files.upload()

#Fetching training data
training_data = pd.read_csv(io.StringIO(uploaded['ex1data1.csv'].decode('utf-8')))

#Scattering training data
plt.scatter(training_data.x, training_data.y, marker='o', c='b')

plt.title('Profits distribution')
plt.xlabel('Population of City in 10,000s')
plt.ylabel('Profit in $10,000s')
plt.show()

#Spliting training data
x_train = training_data.x
y_train = training_data.y

#Reshape training data
x_train = np.array(x_train)
x_train = x_train.reshape(-1,1)
y_train = np.array(y_train)
y_train = y_train.reshape(-1,1)

#number of training samples
m = y_train.size

#Add a column of ones to X (interception data)
x_train = np.c_[np.ones(m), x_train]
#Initialize theta parameters
theta = np.zeros(shape=(2, 1))

#Some gradient descent settings
num_iters = 6000
alpha = 0.02

#Evaluate the linear regression
def compute_cost(x_train, y_train, theta):
    
    #Number of training samples
    m = y_train.size
    
    #Prediction
    predictions = np.dot(x_train,theta)
    
    #Square error difference
    sqErrors = (predictions - y_train) ** 2
    
    #Calculating cost
    J = (1.0 / (2 * m)) * sqErrors.sum()

    return J

# Evaluate Gradient Descent(Minimizing cost function)
def gradient_descent(x_train, y_train, theta, alpha, num_iters):
    m = y_train.size
    
    #Initializing cost hisotry
    J_history = np.zeros(num_iters)
    
    for iter in np.arange(num_iters):
      
        #Hypothesis
        h = x_train.dot(theta)
        
        #Updating theta
        theta = theta - alpha*(1/m)*(x_train.T.dot(h-y_train))
        
        #Storing cost
        J_history[iter] = compute_cost(x_train, y_train, theta)
    return(theta, J_history)

#Testing cost function with theta [0;0]
J = compute_cost(x_train, y_train, theta)
print('cost with theta[0;0]:',J)

#Testing cost function with theta [-1;2]
J = compute_cost(x_train, y_train,[[-1],[2]])
print('cost with theta[-1;2]:',J)
print(x_train.size)
print(y_train.size)

#Optimised theta
theta, J_history = gradient_descent(x_train, y_train, theta, alpha, num_iters)
print(theta)

#Plot the results
result = np.dot(x_train,theta).flatten()
plt.scatter(training_data.x, training_data.y, marker='o', c='b')
plt.plot(training_data.x, result, c = 'r')
plt.show()

#Theta minimizing function
plt.plot(J_history[0:1000])
plt.ylabel('Cost J')
plt.xlabel('Iterations');

theta_best = np.linalg.inv(x_train.T.dot(x_train)).dot(x_train.T).dot(y_train)
print(theta_best)
# X_new = np.array([[0], [2]])
# X_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance
# y_predict = X_new_b.dot(theta_best)

from mpl_toolkits.mplot3d import axes3d
# Create grid coordinates for plotting
B0 = np.linspace(-10, 10, 50)
B1 = np.linspace(-1, 4, 50)
xx, yy = np.meshgrid(B0, B1, indexing='xy')
Z = np.zeros((B0.size,B1.size))

# Calculate Z-values (Cost) based on grid of coefficients
for (i,j),v in np.ndenumerate(Z):
    Z[i,j] = compute_cost(x_train,y_train, theta=[[xx[i,j]], [yy[i,j]]])

fig = plt.figure(figsize=(15,6))
ax1 = fig.add_subplot(121)
ax2 = fig.add_subplot(122, projection='3d')

# Left plot
CS = ax1.contour(xx, yy, Z, np.logspace(-2, 3, 20), cmap=plt.cm.jet)
ax1.scatter(theta[0],theta[1], c='r')

# Right plot
ax2.plot_surface(xx, yy, Z, rstride=1, cstride=1, alpha=0.6, cmap=plt.cm.jet)
ax2.set_zlabel('Cost')
ax2.set_zlim(Z.min(),Z.max())
ax2.view_init(elev=15, azim=230)
# settings common to both plots
for ax in fig.axes:
    ax.set_xlabel(r'$\theta_0$', fontsize=17)
    ax.set_ylabel(r'$\theta_1$', fontsize=17)

#Linear regression with library function

from sklearn.linear_model import LinearRegression

#Creating the object of regression
regr = linear_model.LinearRegression()

#Spliting training data
x1_train = training_data.x
y1_train = training_data.y

#Reshape training data
x1_train = np.array(x1_train)
x1_train = x1_train.reshape(-1,1)
y1_train = np.array(y1_train)
y1_train = y1_train.reshape(-1,1) 

#Train the model using the training sets
regr.fit(x1_train, y1_train)

plt.figure(figsize=(8,6))


#Plot outputs
plt.scatter(x1_train, y1_train, color='black')
plt.title('Profits distribution')
plt.xlabel('Population of City in 10,000s')
plt.ylabel('Profit in $10,000s')
plt.plot(x1_train,regr.predict(x1_train), color='red',linewidth=3)
# predicted = regr.predict(testing_data)
# print(predicted)

